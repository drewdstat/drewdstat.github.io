{
  "hash": "f6906f71141232d099227e01454fd1f6",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"How to use the impgrplasso package\"\nauthor: \"Drew Day\"\ndate: \"12/03/2023\"\nexecute:\n  freeze: auto\ncategories:\n  - impgrplasso\n---\n\n\n\n\n\n# Introduction\n\nLeast Absolute Shrinkage and Selection Operator (LASSO) regression is a \nstatistical technique to impose penalties on independent variables in a \nregression in order to produce a more sparse model to improve out-of-sample \nprediction as well as to enhance the interpretability of the model. The L1 penalty\nof the LASSO model can shrink independent variables to zero, in effect removing \nthem from regression models. There is no clear way of defining traditional \nfrequentist error estimates (i.e., confidence intervals and p-values) for the \ncoefficient estimates in a LASSO regression model since any null distribution\nfor a given independent variable would inevitably result in some models omitting\nthat variable and others not. For this same reason, traditional approaches for \ncombining regression estimates from multiply imputed data do not apply to LASSO\nregression.\n\nMultiple imputation is a series of methods for imputing multiple potential values \nfor a missing observation, often based on a probability distribution that is \npredicted by the other independent and dependent variables to be used in a \nregression incorporating that variable with missingness. Multiple imputation \nwith chained equations (MICE) is a well-established multiple imputation method\nthat begins by imputing the mean for each missing value of each variable with \nmissingness except for one, using the now \"complete\" data of other variables to\npredict values using regression models for the missing values in that selected \nvariable and then use those as imputed values in the next regression imputing \nvalues for the next variable, and so on in several iterations of chained \nregressions. For more explanation see [Azur et al. 2011](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3074241/).\n\nThe goal of this package is to adapt one of several suggested methods for \ncombining multiple imputation and LASSO regression, namely the MI-LASSO method of\n[Chen and Wang 2013](https://onlinelibrary.wiley.com/doi/10.1002/sim.5783), which stacks imputed data sets\nand then uses a group LASSO penalty to jointly penalize each imputed version of\na given independent variable. The 'impgrplasso' package builds off of this method,\nwhich provided the best combination of interpretability and low prediction error\nin the comparative simulation study of [Gunn et al. 2023](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10117422/), by\nincorporating the following additional features:\n\n  - The use of a prediction loss measure for optimizing the lambda parameter as \n  suggested by Gunn et al. 2023 rather than using the Bayes Information \n  Criterion (BIC), adding a log-loss measure for LASSO logistic regression \n  prediction error to the mean squared prediction error (MSE) proposed by Gunn \n  et al. in the case of LASSO standard regression\n  - The incorporation of k-fold cross validation to optimize lambda rather than \n  single training/test splits\n  - The incorporation of the established [‘grplasso’](https://CRAN.R-project.org/package=grplasso) package ([Meier et al. 2008](https://rss.onlinelibrary.wiley.com/doi/10.1111/j.1467-9868.2007.00627.x)) \n  for computing group LASSO penalties\n  - The option to not penalize a subset of independent variables in the LASSO regression\n  - Automatic detection of dummy variables from the same categorical variable and\n  the application of a group LASSO penalty to those variables\n\n## About LASSO Regression\n\nStandard LASSO regression minimizes the loss function below:\n\n$$\n\\sum_{i=1}^{N}{(y_i-[\\beta_0+\\sum_{j=1}^{p}{\\beta_j x_{ij}]})^2} + \\lambda \\sum_{j=1}^{p}{\\left\\lvert{\\beta_j}\\right\\lvert}\n$$\n\nNotice how the formula to the left of the addition sign is just the function that \nan ordinary least squares (OLS) regression minimizes (i.e., the sum of the squared\nresiduals). The right of the addition sign is the penalization term, which is\nthe sum of the absolute values of all independent variable coefficients $\\beta_j$\nscaled by the parameter $\\lambda$.\n\n## About MI-LASSO\n\nThe Chen and Wang 2013 MI-LASSO approach to LASSO regression for multiply imputed\ndata minimizes the similar function below:\n\n$$\n\\sum_{d=1}^{m}{\\sum_{i=1}^{N}{(y_{di}-[\\beta_{0d}+\\sum_{j=1}^{p}{\\beta_{dj} x_{dij}}])^2}} + \\lambda \\sum_{j=1}^{p}\\sqrt{\\sum_{d=1}^{m}{\\beta^2_{dj}}}\n$$\n\nHere the loss function is being summed over $m$ imputed data sets. In practice, we\nstack the imputed data sets into a model matrix in a diagonal fashion, such that \neach imputed version of a variable gets a unique column and there are columns of \nones for each imputed data set-specific intercept. Then the rows are the \nobservations repeated $m$ times with zero values for all but the relevant $d$th \ncolumn for each variable. As an example, let's say I have 3 observations and 2 \nvariables ($X1$ and $X2$) with two imputed data sets. $X1$ is continuous, but \n$X2$ is categorical and has three levels, meaning that two dummy variables will \nbe generated (the referent level is omitted). Using the vertical bar to denote\nthe imputed version of each variable and the underscore to denote levels of a\ncategorical variable, I will get the following model matrix:\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> ID </th>\n   <th style=\"text-align:right;\"> beta0|1 </th>\n   <th style=\"text-align:right;\"> X1|1 </th>\n   <th style=\"text-align:right;\"> X2_Level1|1 </th>\n   <th style=\"text-align:right;\"> X2_Level2|1 </th>\n   <th style=\"text-align:right;\"> beta0|2 </th>\n   <th style=\"text-align:right;\"> X1|2 </th>\n   <th style=\"text-align:right;\"> X2_Level1|2 </th>\n   <th style=\"text-align:right;\"> X2_Level2|2 </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> ID1 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 6.5 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0.0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> ID2 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0.2 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0.0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> ID3 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 3.3 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0.0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> ID1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0.0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 5.9 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> ID2 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0.0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0.2 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> ID3 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0.0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 3.1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n\nThe parameter $\\lambda$ has to be chosen a priori before each model run, and as\n$\\lambda$ increases, the degree of penalization increases. Using k-fold cross-\nvalidation, a $\\lambda$ value that minimizes out-of-sample prediction loss \nfunction can be determined. The range of possible $\\lambda$ will be higher than \nfor a typical LASSO regression because of the high number of imputed columns and \nthe group LASSO penalty between them.\n\n# How to use the `impgrplasso` package\n\nTo use `impgrplasso`, we must first generate a list of imputed data frames. This \ncan be done for example using the `mice` function from the 'mice' package or \nusing the function `mice_by_group` from this package. The list of data frames \nmust have all categorical columns transformed into dummy variables. This can \neither be done beforehand or the user can set the argument 'dummify' to TRUE \n(the default), in which case the function will do that transformation. Note that \nfor proper functioning, variable names should not contain underscores (\"_\") so \nthat when the dummy variables are generated, an underscore can be added by \n`impgrplasso` to separate the variable name (before the added underscore) from \nits respective levels (after the added underscore).\n\nIn this example, we will use MICE and create a list of imputed data frames using\nthe `nhanes` example data frame included in the 'mice' package.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsuppressPackageStartupMessages(library(mice))\nnh <- nhanes\nnh$hyp <- as.factor(nh$hyp - 1)\n# above brings \"hyp\" from a {1, 2} set of unique values to {0, 1}\n\nimp <- mice(nh, printFlag = FALSE)\ndlist <- lapply(1:imp$m, function(x) complete(imp, x))\n```\n:::\n\n\n\n## mice_by_group\n\nAnother option is to use the function `mice_by_group` to generate group-specific\nimputed data, for example for imputing within study cohorts or study sites in a \nmulti-cohort study.\n\n### Arguments\n\n* `Data`: A data frame containing all variables (columns) to be used in \nthe MICE equations and a column with the grouping variable.\n* `groupvar`: The column name in `Data` for the grouping variable. If\nthis column is not of the class 'factor', it will be converted to a factor\nwith the default order of factor levels.\n* `miceArgs`: These are arguments to be passed on to the `mice` function from \nthe 'mice' package. These include the number of imputed data frames to be \ngenerated `m` (default = 10), the maximum number of MICE iterations `maxit` \n(default = 10), a random seed to be set for the MICE for reproducibility `seed` \n(default = 123), and a Boolean option for verbose printing `printFlag` (default \n= FALSE). All other `mice` function arguments default to the same values as in \nthe 'mice' package (see `?mice::mice` for details). Note that the default `m` and \n`maxit` for `mice` are both 5, though I set them to 10 for this function for \nmore iterations and better convergence, respectively.  \n\n### Outputs\n\nThe output of `mice_by_group` is a list of imputed data frames.\n\n### Examples\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(mice)\nlibrary(impgrplasso)\n\nnh$Group <- as.factor(c(rep(LETTERS[1:2], times = c(6, 7)), \n                        rep(LETTERS[1:2], times = c(6, 6))))\n\ndlist2 <-  mice_by_group(nh, \"Group\", list(m = 5, maxit = 5, seed = 2, \n                                           printFlag = FALSE))\n#> Warning: Number of logged events: 25\n```\n:::\n\n\n\n## impgrplasso\n\nNow that we have a list of imputed data frames, we can use impgrplasso.\n\n### Arguments\n\n * `impdatlist`: A list of data frames generated by multiple imputation.\n * `lams`: Either a single lambda value or a vector of lambda values. If one\n value is provided, the function will perform the MI-LASSO on the full data\n using only that single lambda value. If a vector of values is provided, the\n function will perform cross-validation to obtain fold- and imputation-specific\n as well as averaged loss values and coefficients for each lambda value so \n that an optimal lambda can be chosen.\n * `outname`: A column name for the dependent variable. This can be continuous\n or binary. In the case of the latter, a LASSO logistic regression will be \n performed.\n * `prednames`: Column names for the independent variables. These should be\n free of underscores. If dummy variables have already been made for the list\n of data frames `impdatlist`, provide those names with underscores between\n the variable name and the level (e.g., \"sex_male\"). \n * `forcedin`: This is an optional vector of column names that should be a \n subset of `prednames` that identifies unpenalized variables to be\n \"forced in\" to the model due to them not experiencing any shrinkage.\n * `kfolds`: If a vector of lambda values is provided for the argument `lams`, \n `kfolds` provides the number of cross-validation folds for\n the cross-validation process. This defaults to 10.\n * `kfoldseed`: The rows chosen for each k-fold cross-validation are randomly\n selected once at the beginning of the function and then this selection is used\n consistently throughout subsequent steps. This value sets a seed for that \n random process. This defaults to 10.\n * `scalecenter`: If TRUE (default), the function will scale and center all\n variables prior to calculations by subtracting the means of each variable and \n dividing by the standard deviations. This is recommended for LASSO regression,\n but one can also perform their own form of standardization in the list of data\n frames and set this to FALSE if they prefer another standardization approach.\n * `dummify`: If TRUE (default), the function will produce dummy variables\n for all variables in `prednames` of classes 'factor' or 'character'. This\n can be set to FALSE if one wants to create dummy variables ahead of time and \n include them in the list of data frames `impdatlist`.\n\n### Outputs\n\n`impgrplasso` outputs a list of class 'impgrplasso' with the following items:\n\n * `Coef`: If `lams` is a vector, this is a list of fold-specific data frames \n containing coefficients for each variable for specific imputed data frames and \n lambda values. If `lams` is a single value, this is a single data frame with \n variable- and imputation-specific coefficients.\n * `MeanCoef`: If `lams` is a vector of length > 1, this is a data frame of mean \n coefficients across imputed data frames for each variable at each fold and \n lambda value. If `lams` is a single value, this is a data frame with mean \n coefficients across imputed data frames for each variable.\n * `Loss`: This appears only if `lams` is a vector of length > 1, and it is a \n data frame showing fold- and imputation-specific loss values and means for each \n value of lambda.\n * `MeanLoss`: This appears only if `lams` is a vector of length > 1, and it is \n a data frame showing the mean prediction loss for each lambda value.\n * `Model`: This appears only if `lams` is a single value, and it is the grplasso \n model object run on the full data.\n * `Index`: This is the index argument for the grplasso function generated by \n impgrplasso.\n * `allX`: This appears only if `lams` is a single value, and it is the large \n stacked matrix of independent variables across imputed data frames generated by \n `impgrplasso`.\n\nA summary method is available for this output, either providing the average \nprediction loss per lambda value if multiple lambda values were input or the \nLASSO regression coefficients averaged over imputed data sets if one lambda value\nwas input.\n\n### Examples\n\nNow that we have our imputed datalist `dlist`, we can perform two steps to \nobtain our optimal LASSO model with those multiply imputed datasets. First we \nneed to search for the optimal lambda value over a range of possible values, and \nso we set `lams` to be a vector of values, here `seq(1, 3, 1)`. Notice how we \nforce the covariate `\"age\"` to have no shrinkage in the model runs by setting \n`forcedin = \"age\"` for this example. We would \"force in\" covariates if we \nstrongly believe that they are confounders and that shrinking them would bias \nour results.\n\nOnce we've run the multiple lasso regressions (saved as `multiple_milasso` here), \nwe use `summary()` to print out a summary of the optimal lambda value. \nNotice that the lambda values of 2 and 3 both produce identical numbers of \nvariables kept in the model (i.e., dropping `\"bmi\"` and `\"chl\"`) in almost \nevery case. Since the variables kept in the model are the \"unshrunk\" intercept \nand `\"age\"`, the loss values are also identical for those lambda values in \nthose cass. Moving forward, we choose the lambda value with the lowest mean \nloss, which is 3.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\n# Searching for the best lambda by running multiple LASSO regressions\n\nmultiple_milasso <- impgrplasso(impdatlist = dlist, lams = seq(1, 3, 1), \noutname = \"hyp\", prednames = c(\"age\", \"bmi\", \"chl\"), forcedin = \"age\", \nkfolds = 4, kfoldseed = 12)\n#> [1] \"Fold 1\"\n#> Lambda: 1  nr.var: 20 \n#> Lambda: 2  nr.var: 10 \n#> Lambda: 3  nr.var: 10 \n#> [1] \"Fold 2\"\n#> Lambda: 1  nr.var: 20 \n#> Lambda: 2  nr.var: 15 \n#> Lambda: 3  nr.var: 10 \n#> [1] \"Fold 3\"\n#> Lambda: 1  nr.var: 15 \n#> Lambda: 2  nr.var: 10 \n#> Lambda: 3  nr.var: 10 \n#> [1] \"Fold 4\"\n#> Lambda: 1  nr.var: 15 \n#> Lambda: 2  nr.var: 15 \n#> Lambda: 3  nr.var: 10\n\nsummary(multiple_milasso)\n#> Mean prediction loss by lambda:\n#>         1         2         3 \n#> 0.6866787 0.6736998 0.6708432\n```\n:::\n\n\n\nWe can plot the prediction loss for each lambda value, multiply imputed \ndataset (\"ImpX\"), and LASSO regression fold iteration as follows:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\n\nplotdat <- reshape2::melt(multiple_milasso$Loss, id.vars = c(\"Fold\", \"Lambda\"),\n                          measure.vars = paste0(\"Imp\", 1:5), \n                          variable.name = \"Imp\", value.name = \"MeanLoss\")\nplotdat$Fold <- as.factor(plotdat$Fold)\nlevels(plotdat$Fold) <- paste0(\"Fold \", levels(plotdat$Fold))\nplotdat$Lambda <- as.factor(plotdat$Lambda)\n\nggplot(plotdat, aes(x=Imp, y=MeanLoss, color=Lambda)) + theme_bw() + \n  geom_line(aes(group = Lambda)) + geom_point(size = 2) + facet_grid(~ Fold) + \n  scale_color_brewer(palette = \"Set1\") + ylab (\"Mean Loss\") +\n  theme(strip.text = element_text(size = 12))\n```\n\n::: {.cell-output-display}\n![](impgrplasso_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n\nWe then apply that optimal lambda in a second step to a single, optimal LASSO \nregression run, which represents our final results. In order to have a single \nrun of the `impgrplasso` function, we simply set `lams` to be a single number. \nAgain `\"age\"` is \"forced in\" for this model. The final results are summarized \nby calling the `summary()` function.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Now with the optimal lambda determined from the last step, we do a single run\n# with that optimal lambda\n\nsinglerun_milasso <- impgrplasso(impdatlist = dlist, lams = 3, outname = \"hyp\", \n                                 prednames = c(\"age\", \"bmi\", \"chl\"), \n                                 forcedin = \"age\")\n#> Lambda: 3  nr.var: 15\nsummary(singlerun_milasso)\n#> Mean LASSO regression coefficients:\n#>         beta0           age           bmi           chl \n#> -2.2883132551  1.4702719276  0.0004690066  0.0000000000\n```\n:::\n",
    "supporting": [
      "impgrplasso_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"../site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}