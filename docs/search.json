[
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "My Research",
    "section": "",
    "text": "Google Scholar\n  \n  \n    \n     Full CV\n  \n\n  \n  \n\nAbout Me\nI’m an epidemiologist and statistician formerly working in chemical safety and chronic disease research at the Seattle Children’s Research Institute, and now looking for opportunities in pharmaceutical research and development.\nMy primary experience is in both developing and applying statistical methods to investigate the determinants of chronic disease. I have training in Bayesian and frequentist statistics, machine learning, epidemiology, toxicology, immunology, bioinformatics (transcriptomics, scRNA, metabolomics, etc.), endocrinology, and neurology.\n\n\n\n\n\n\nMethods Development/Teaching\n\nR package wqspt: Using a novel permutation test method to improve the statistical properties of the weighted quantile sum regression, which determines the additive combined association of multiple exposures with a single outcome.\nR package ResidualKMeans: Method for unsupervised clustering of mixed type continuous and categorical data while controlling for structure induced by a grouping variable such as cohort in a multi-cohort study.\nR package impgrplasso: Combining methods involving a group LASSO penalty for multiply imputed data into one function to perform LASSO regression on data with missingness.\nR package DrewDayRFunctions: Functions with relevance for epidemiologic analyses including functions to performing many regressions, nonlinear mediation, nonlinear regressions, and exploratory data analysis.\nR teaching Introduction to R: Document to teach a student just starting with R to go from zero to developing useful functions and loops.\n\n\n\nResearch Interests\n\nOmics prediction of chronic disease: Using omics data to predict chronic disease patterns to discover shared mechanisms (Placental gene transcription and child cognitive and behavioral outcomes (abstract))\nMulti-outcome health profiles: Using phenotypes of disease outcome co-occurrence to indentify risks and mechanisms shared between seemingly unrelated diseases (Residual K-means paper) (see also my experiments with autoencoders at VAECM_Final)\nMixture exposures: Advancing mixture exposure modelling to understand the combined health impact of common environmental factors (Weighted Quantile Sum regression with Permutation Test (WQSPT) methods paper)\nEnvironmental Epidemiology: exploring associations between chemical and other stressor exposures and later life chronic disease:\n\nChildhood melamine and kidney health outcomes\nPrenatal phthalate plasticizers and child behavioral disorders\nPrenatal maternal sex hormones and child behavioral disorders\nOzone air pollution and cardiopulmonary health in adults\nElectrostatic precipitator air filtration and cardiopulmonary health in adults"
  },
  {
    "objectID": "posts/econml_customerseg.html",
    "href": "posts/econml_customerseg.html",
    "title": "Experiments with EconML and Causal Learning: Customer Segmentation",
    "section": "",
    "text": "This document will cover practical examples of the use of causal machine learning in the context of business strategy. It draws heavily from and then expands on the “Customer Scenarios” included in the EconML Python package documentation."
  },
  {
    "objectID": "posts/econml_customerseg.html#introduction",
    "href": "posts/econml_customerseg.html#introduction",
    "title": "Experiments with EconML and Causal Learning: Customer Segmentation",
    "section": "",
    "text": "This document will cover practical examples of the use of causal machine learning in the context of business strategy. It draws heavily from and then expands on the “Customer Scenarios” included in the EconML Python package documentation."
  },
  {
    "objectID": "posts/econml_customerseg.html#case-study-1-customer-segmentation",
    "href": "posts/econml_customerseg.html#case-study-1-customer-segmentation",
    "title": "Experiments with EconML and Causal Learning: Customer Segmentation",
    "section": "Case Study 1: Customer Segmentation",
    "text": "Case Study 1: Customer Segmentation\nWhen developing a strategy to\n\nGetting started: Importing and cleaning data\n\n\nSource Code\n#key imports\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom econml.dml import LinearDML, CausalForestDML\nfrom econml.cate_interpreter import SingleTreeCateInterpreter, SingleTreePolicyInterpreter\n\n#Load data\nfile_url = \"https://msalicedatapublic.z5.web.core.windows.net/datasets/Pricing/pricing_sample.csv\"\ntrain_data = pd.read_csv(file_url)\n\ntrain_data.head()\n\n\n   account_age  age  avg_hours  ...    income  price     demand\n0            3   53   1.834234  ...  0.960863    1.0   3.917117\n1            5   54   7.171411  ...  0.732487    1.0  11.585706\n2            3   33   5.351920  ...  1.130937    1.0  24.675960\n3            2   34   6.723551  ...  0.929197    1.0   6.361776\n4            4   30   2.448247  ...  0.533527    0.8  12.624123\n\n[5 rows x 11 columns]\n\n\n\n\nSource Code\n#Define estimator inputs\nY = train_data[\"demand\"] #main outcome\nT = train_data[\"price\"] #treatment\nX = train_data[[\"income\"]] #features\nW = train_data.drop(columns = [\"demand\", \"price\", \"income\"]) #covariates\n\n# Get test data\nX_test = np.linspace(0, 5, 100).reshape(-1, 1)\nX_test_data = pd.DataFrame(X_test, columns = [\"income\"])"
  },
  {
    "objectID": "pkgguides.html",
    "href": "pkgguides.html",
    "title": "My Packages",
    "section": "",
    "text": "DrewDayRFunctions: An Overview\n\n\n\n\n\n\nDrewDayRFunctions\n\n\n\n\n\n\n\n\n\nDec 20, 2024\n\n\nDrew Day\n\n\n\n\n\n\n\n\n\n\n\n\nHow to use the impgrplasso package\n\n\n\n\n\n\nimpgrplasso\n\n\n\n\n\n\n\n\n\nDec 3, 2023\n\n\nDrew Day\n\n\n\n\n\n\n\n\n\n\n\n\nHow to use the wqspt package\n\n\n\n\n\n\nwqspt\n\n\n\n\n\n\n\n\n\nMay 26, 2022\n\n\nDrew Day, James Peng\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "pkgguides/impgrplasso.html",
    "href": "pkgguides/impgrplasso.html",
    "title": "How to use the impgrplasso package",
    "section": "",
    "text": "Least Absolute Shrinkage and Selection Operator (LASSO) regression is a statistical technique to impose penalties on independent variables in a regression in order to produce a more sparse model to improve out-of-sample prediction as well as to enhance the interpretability of the model. The L1 penalty of the LASSO model can shrink independent variables to zero, in effect removing them from regression models. There is no clear way of defining traditional frequentist error estimates (i.e., confidence intervals and p-values) for the coefficient estimates in a LASSO regression model since any null distribution for a given independent variable would inevitably result in some models omitting that variable and others not. For this same reason, traditional approaches for combining regression estimates from multiply imputed data do not apply to LASSO regression.\nMultiple imputation is a series of methods for imputing multiple potential values for a missing observation, often based on a probability distribution that is predicted by the other independent and dependent variables to be used in a regression incorporating that variable with missingness. Multiple imputation with chained equations (MICE) is a well-established multiple imputation method that begins by imputing the mean for each missing value of each variable with missingness except for one, using the now “complete” data of other variables to predict values using regression models for the missing values in that selected variable and then use those as imputed values in the next regression imputing values for the next variable, and so on in several iterations of chained regressions. For more explanation see Azur et al. 2011.\nThe goal of this package is to adapt one of several suggested methods for combining multiple imputation and LASSO regression, namely the MI-LASSO method of Chen and Wang 2013, which stacks imputed data sets and then uses a group LASSO penalty to jointly penalize each imputed version of a given independent variable. The ‘impgrplasso’ package builds off of this method, which provided the best combination of interpretability and low prediction error in the comparative simulation study of Gunn et al. 2023, by incorporating the following additional features:\n\nThe use of a prediction loss measure for optimizing the lambda parameter as suggested by Gunn et al. 2023 rather than using the Bayes Information Criterion (BIC), adding a log-loss measure for LASSO logistic regression prediction error to the mean squared prediction error (MSE) proposed by Gunn et al. in the case of LASSO standard regression\nThe incorporation of k-fold cross validation to optimize lambda rather than single training/test splits\nThe incorporation of the established ‘grplasso’ package (Meier et al. 2008) for computing group LASSO penalties\nThe option to not penalize a subset of independent variables in the LASSO regression\nAutomatic detection of dummy variables from the same categorical variable and the application of a group LASSO penalty to those variables\n\n\n\nStandard LASSO regression minimizes the loss function below:\n\\[\n\\sum_{i=1}^{N}{(y_i-[\\beta_0+\\sum_{j=1}^{p}{\\beta_j x_{ij}]})^2} + \\lambda \\sum_{j=1}^{p}{\\left\\lvert{\\beta_j}\\right\\lvert}\n\\]\nNotice how the formula to the left of the addition sign is just the function that an ordinary least squares (OLS) regression minimizes (i.e., the sum of the squared residuals). The right of the addition sign is the penalization term, which is the sum of the absolute values of all independent variable coefficients \\(\\beta_j\\) scaled by the parameter \\(\\lambda\\).\n\n\n\nThe Chen and Wang 2013 MI-LASSO approach to LASSO regression for multiply imputed data minimizes the similar function below:\n\\[\n\\sum_{d=1}^{m}{\\sum_{i=1}^{N}{(y_{di}-[\\beta_{0d}+\\sum_{j=1}^{p}{\\beta_{dj} x_{dij}}])^2}} + \\lambda \\sum_{j=1}^{p}\\sqrt{\\sum_{d=1}^{m}{\\beta^2_{dj}}}\n\\]\nHere the loss function is being summed over \\(m\\) imputed data sets. In practice, we stack the imputed data sets into a model matrix in a diagonal fashion, such that each imputed version of a variable gets a unique column and there are columns of ones for each imputed data set-specific intercept. Then the rows are the observations repeated \\(m\\) times with zero values for all but the relevant \\(d\\)th column for each variable. As an example, let’s say I have 3 observations and 2 variables (\\(X1\\) and \\(X2\\)) with two imputed data sets. \\(X1\\) is continuous, but \\(X2\\) is categorical and has three levels, meaning that two dummy variables will be generated (the referent level is omitted). Using the vertical bar to denote the imputed version of each variable and the underscore to denote levels of a categorical variable, I will get the following model matrix:\n\n\n\n\n\nID\nbeta0|1\nX1|1\nX2_Level1|1\nX2_Level2|1\nbeta0|2\nX1|2\nX2_Level1|2\nX2_Level2|2\n\n\n\n\nID1\n1\n6.5\n1\n1\n0\n0.0\n0\n0\n\n\nID2\n1\n0.2\n0\n0\n0\n0.0\n0\n0\n\n\nID3\n1\n3.3\n1\n1\n0\n0.0\n0\n0\n\n\nID1\n0\n0.0\n0\n0\n1\n5.9\n1\n1\n\n\nID2\n0\n0.0\n0\n0\n1\n0.2\n1\n0\n\n\nID3\n0\n0.0\n0\n0\n1\n3.1\n0\n1\n\n\n\n\n\n\n\nThe parameter \\(\\lambda\\) has to be chosen a priori before each model run, and as \\(\\lambda\\) increases, the degree of penalization increases. Using k-fold cross- validation, a \\(\\lambda\\) value that minimizes out-of-sample prediction loss function can be determined. The range of possible \\(\\lambda\\) will be higher than for a typical LASSO regression because of the high number of imputed columns and the group LASSO penalty between them."
  },
  {
    "objectID": "pkgguides/impgrplasso.html#about-lasso-regression",
    "href": "pkgguides/impgrplasso.html#about-lasso-regression",
    "title": "How to use the impgrplasso package",
    "section": "",
    "text": "Standard LASSO regression minimizes the loss function below:\n\\[\n\\sum_{i=1}^{N}{(y_i-[\\beta_0+\\sum_{j=1}^{p}{\\beta_j x_{ij}]})^2} + \\lambda \\sum_{j=1}^{p}{\\left\\lvert{\\beta_j}\\right\\lvert}\n\\]\nNotice how the formula to the left of the addition sign is just the function that an ordinary least squares (OLS) regression minimizes (i.e., the sum of the squared residuals). The right of the addition sign is the penalization term, which is the sum of the absolute values of all independent variable coefficients \\(\\beta_j\\) scaled by the parameter \\(\\lambda\\)."
  },
  {
    "objectID": "pkgguides/impgrplasso.html#about-mi-lasso",
    "href": "pkgguides/impgrplasso.html#about-mi-lasso",
    "title": "How to use the impgrplasso package",
    "section": "",
    "text": "The Chen and Wang 2013 MI-LASSO approach to LASSO regression for multiply imputed data minimizes the similar function below:\n\\[\n\\sum_{d=1}^{m}{\\sum_{i=1}^{N}{(y_{di}-[\\beta_{0d}+\\sum_{j=1}^{p}{\\beta_{dj} x_{dij}}])^2}} + \\lambda \\sum_{j=1}^{p}\\sqrt{\\sum_{d=1}^{m}{\\beta^2_{dj}}}\n\\]\nHere the loss function is being summed over \\(m\\) imputed data sets. In practice, we stack the imputed data sets into a model matrix in a diagonal fashion, such that each imputed version of a variable gets a unique column and there are columns of ones for each imputed data set-specific intercept. Then the rows are the observations repeated \\(m\\) times with zero values for all but the relevant \\(d\\)th column for each variable. As an example, let’s say I have 3 observations and 2 variables (\\(X1\\) and \\(X2\\)) with two imputed data sets. \\(X1\\) is continuous, but \\(X2\\) is categorical and has three levels, meaning that two dummy variables will be generated (the referent level is omitted). Using the vertical bar to denote the imputed version of each variable and the underscore to denote levels of a categorical variable, I will get the following model matrix:\n\n\n\n\n\nID\nbeta0|1\nX1|1\nX2_Level1|1\nX2_Level2|1\nbeta0|2\nX1|2\nX2_Level1|2\nX2_Level2|2\n\n\n\n\nID1\n1\n6.5\n1\n1\n0\n0.0\n0\n0\n\n\nID2\n1\n0.2\n0\n0\n0\n0.0\n0\n0\n\n\nID3\n1\n3.3\n1\n1\n0\n0.0\n0\n0\n\n\nID1\n0\n0.0\n0\n0\n1\n5.9\n1\n1\n\n\nID2\n0\n0.0\n0\n0\n1\n0.2\n1\n0\n\n\nID3\n0\n0.0\n0\n0\n1\n3.1\n0\n1\n\n\n\n\n\n\n\nThe parameter \\(\\lambda\\) has to be chosen a priori before each model run, and as \\(\\lambda\\) increases, the degree of penalization increases. Using k-fold cross- validation, a \\(\\lambda\\) value that minimizes out-of-sample prediction loss function can be determined. The range of possible \\(\\lambda\\) will be higher than for a typical LASSO regression because of the high number of imputed columns and the group LASSO penalty between them."
  },
  {
    "objectID": "pkgguides/impgrplasso.html#mice_by_group",
    "href": "pkgguides/impgrplasso.html#mice_by_group",
    "title": "How to use the impgrplasso package",
    "section": "mice_by_group",
    "text": "mice_by_group\nAnother option is to use the function mice_by_group to generate group-specific imputed data, for example for imputing within study cohorts or study sites in a multi-cohort study.\n\nArguments\n\nData: A data frame containing all variables (columns) to be used in the MICE equations and a column with the grouping variable.\ngroupvar: The column name in Data for the grouping variable. If this column is not of the class ‘factor’, it will be converted to a factor with the default order of factor levels.\nmiceArgs: These are arguments to be passed on to the mice function from the ‘mice’ package. These include the number of imputed data frames to be generated m (default = 10), the maximum number of MICE iterations maxit (default = 10), a random seed to be set for the MICE for reproducibility seed (default = 123), and a Boolean option for verbose printing printFlag (default = FALSE). All other mice function arguments default to the same values as in the ‘mice’ package (see ?mice::mice for details). Note that the default m and maxit for mice are both 5, though I set them to 10 for this function for more iterations and better convergence, respectively.\n\n\n\nOutputs\nThe output of mice_by_group is a list of imputed data frames.\n\n\nExamples\n\nlibrary(mice)\nlibrary(impgrplasso)\n\nnh$Group &lt;- as.factor(c(rep(LETTERS[1:2], times = c(6, 7)), \n                        rep(LETTERS[1:2], times = c(6, 6))))\n\ndlist2 &lt;-  mice_by_group(nh, \"Group\", list(m = 5, maxit = 5, seed = 2, \n                                           printFlag = FALSE))\n#&gt; Warning: Number of logged events: 25"
  },
  {
    "objectID": "pkgguides/impgrplasso.html#impgrplasso",
    "href": "pkgguides/impgrplasso.html#impgrplasso",
    "title": "How to use the impgrplasso package",
    "section": "impgrplasso",
    "text": "impgrplasso\nNow that we have a list of imputed data frames, we can use impgrplasso.\n\nArguments\n\nimpdatlist: A list of data frames generated by multiple imputation.\nlams: Either a single lambda value or a vector of lambda values. If one value is provided, the function will perform the MI-LASSO on the full data using only that single lambda value. If a vector of values is provided, the function will perform cross-validation to obtain fold- and imputation-specific as well as averaged loss values and coefficients for each lambda value so that an optimal lambda can be chosen.\noutname: A column name for the dependent variable. This can be continuous or binary. In the case of the latter, a LASSO logistic regression will be performed.\nprednames: Column names for the independent variables. These should be free of underscores. If dummy variables have already been made for the list of data frames impdatlist, provide those names with underscores between the variable name and the level (e.g., “sex_male”).\nforcedin: This is an optional vector of column names that should be a subset of prednames that identifies unpenalized variables to be “forced in” to the model due to them not experiencing any shrinkage.\nkfolds: If a vector of lambda values is provided for the argument lams, kfolds provides the number of cross-validation folds for the cross-validation process. This defaults to 10.\nkfoldseed: The rows chosen for each k-fold cross-validation are randomly selected once at the beginning of the function and then this selection is used consistently throughout subsequent steps. This value sets a seed for that random process. This defaults to 10.\nscalecenter: If TRUE (default), the function will scale and center all variables prior to calculations by subtracting the means of each variable and dividing by the standard deviations. This is recommended for LASSO regression, but one can also perform their own form of standardization in the list of data frames and set this to FALSE if they prefer another standardization approach.\ndummify: If TRUE (default), the function will produce dummy variables for all variables in prednames of classes ‘factor’ or ‘character’. This can be set to FALSE if one wants to create dummy variables ahead of time and include them in the list of data frames impdatlist.\n\n\n\nOutputs\nimpgrplasso outputs a list of class ‘impgrplasso’ with the following items:\n\nCoef: If lams is a vector, this is a list of fold-specific data frames containing coefficients for each variable for specific imputed data frames and lambda values. If lams is a single value, this is a single data frame with variable- and imputation-specific coefficients.\nMeanCoef: If lams is a vector of length &gt; 1, this is a data frame of mean coefficients across imputed data frames for each variable at each fold and lambda value. If lams is a single value, this is a data frame with mean coefficients across imputed data frames for each variable.\nLoss: This appears only if lams is a vector of length &gt; 1, and it is a data frame showing fold- and imputation-specific loss values and means for each value of lambda.\nMeanLoss: This appears only if lams is a vector of length &gt; 1, and it is a data frame showing the mean prediction loss for each lambda value.\nModel: This appears only if lams is a single value, and it is the grplasso model object run on the full data.\nIndex: This is the index argument for the grplasso function generated by impgrplasso.\nallX: This appears only if lams is a single value, and it is the large stacked matrix of independent variables across imputed data frames generated by impgrplasso.\n\nA summary method is available for this output, either providing the average prediction loss per lambda value if multiple lambda values were input or the LASSO regression coefficients averaged over imputed data sets if one lambda value was input.\n\n\nExamples\nNow that we have our imputed datalist dlist, we can perform two steps to obtain our optimal LASSO model with those multiply imputed datasets. First we need to search for the optimal lambda value over a range of possible values, and so we set lams to be a vector of values, here seq(1, 3, 1). Notice how we force the covariate \"age\" to have no shrinkage in the model runs by setting forcedin = \"age\" for this example. We would “force in” covariates if we strongly believe that they are confounders and that shrinking them would bias our results.\nOnce we’ve run the multiple lasso regressions (saved as multiple_milasso here), we use summary() to print out a summary of the optimal lambda value. Notice that the lambda values of 2 and 3 both produce identical numbers of variables kept in the model (i.e., dropping \"bmi\" and \"chl\") in almost every case. Since the variables kept in the model are the “unshrunk” intercept and \"age\", the loss values are also identical for those lambda values in those cass. Moving forward, we choose the lambda value with the lowest mean loss, which is 3.\n\n\n# Searching for the best lambda by running multiple LASSO regressions\n\nmultiple_milasso &lt;- impgrplasso(impdatlist = dlist, lams = seq(1, 3, 1), \noutname = \"hyp\", prednames = c(\"age\", \"bmi\", \"chl\"), forcedin = \"age\", \nkfolds = 4, kfoldseed = 12)\n#&gt; [1] \"Fold 1\"\n#&gt; Lambda: 1  nr.var: 20 \n#&gt; Lambda: 2  nr.var: 10 \n#&gt; Lambda: 3  nr.var: 10 \n#&gt; [1] \"Fold 2\"\n#&gt; Lambda: 1  nr.var: 20 \n#&gt; Lambda: 2  nr.var: 15 \n#&gt; Lambda: 3  nr.var: 10 \n#&gt; [1] \"Fold 3\"\n#&gt; Lambda: 1  nr.var: 15 \n#&gt; Lambda: 2  nr.var: 10 \n#&gt; Lambda: 3  nr.var: 10 \n#&gt; [1] \"Fold 4\"\n#&gt; Lambda: 1  nr.var: 15 \n#&gt; Lambda: 2  nr.var: 15 \n#&gt; Lambda: 3  nr.var: 10\n\nsummary(multiple_milasso)\n#&gt; Mean prediction loss by lambda:\n#&gt;         1         2         3 \n#&gt; 0.6866787 0.6736998 0.6708432\n\nWe can plot the prediction loss for each lambda value, multiply imputed dataset (“ImpX”), and LASSO regression fold iteration as follows:\n\nlibrary(ggplot2)\n\nplotdat &lt;- reshape2::melt(multiple_milasso$Loss, id.vars = c(\"Fold\", \"Lambda\"),\n                          measure.vars = paste0(\"Imp\", 1:5), \n                          variable.name = \"Imp\", value.name = \"MeanLoss\")\nplotdat$Fold &lt;- as.factor(plotdat$Fold)\nlevels(plotdat$Fold) &lt;- paste0(\"Fold \", levels(plotdat$Fold))\nplotdat$Lambda &lt;- as.factor(plotdat$Lambda)\n\nggplot(plotdat, aes(x=Imp, y=MeanLoss, color=Lambda)) + theme_bw() + \n  geom_line(aes(group = Lambda)) + geom_point(size = 2) + facet_grid(~ Fold) + \n  scale_color_brewer(palette = \"Set1\") + ylab (\"Mean Loss\") +\n  theme(strip.text = element_text(size = 12))\n\n\n\n\n\n\n\n\nWe then apply that optimal lambda in a second step to a single, optimal LASSO regression run, which represents our final results. In order to have a single run of the impgrplasso function, we simply set lams to be a single number. Again \"age\" is “forced in” for this model. The final results are summarized by calling the summary() function.\n\n# Now with the optimal lambda determined from the last step, we do a single run\n# with that optimal lambda\n\nsinglerun_milasso &lt;- impgrplasso(impdatlist = dlist, lams = 3, outname = \"hyp\", \n                                 prednames = c(\"age\", \"bmi\", \"chl\"), \n                                 forcedin = \"age\")\n#&gt; Lambda: 3  nr.var: 15\nsummary(singlerun_milasso)\n#&gt; Mean LASSO regression coefficients:\n#&gt;         beta0           age           bmi           chl \n#&gt; -2.2883132551  1.4702719276  0.0004690066  0.0000000000"
  },
  {
    "objectID": "mountains.html",
    "href": "mountains.html",
    "title": "Mountaineering",
    "section": "",
    "text": "Peakbagger\n  \n\n  \n  \n\nI developed a love for hiking after my first stint living and studying in China. I didn’t really start hiking much until grad school in North Carolina and then afterwords during nearly a year of traveling the world. My five and a half years in Seattle really got me into more challenging and technical mountaineering. Now that I live on the East Coast, my mountaineering and hiking have slowed, but I’m still exploring what the eastern US has to offer.\nSee my Peakbagger page linked here for more information on my summits, and feel free to add me as a friend on there and reach out if you want to hike in the Northeast!"
  },
  {
    "objectID": "mountains.html#background",
    "href": "mountains.html#background",
    "title": "Mountaineering",
    "section": "",
    "text": "I developed a love for hiking after my first stint living and studying in China. I didn’t really start hiking much until grad school in North Carolina and then afterwords during nearly a year of traveling the world. My five and a half years in Seattle really got me into more challenging and technical mountaineering. Now that I live on the East Coast, my mountaineering and hiking have slowed, but I’m still exploring what the eastern US has to offer.\nSee my Peakbagger page linked here for more information on my summits, and feel free to add me as a friend on there and reach out if you want to hike in the Northeast!"
  },
  {
    "objectID": "blogposts.html",
    "href": "blogposts.html",
    "title": "Statistics Blog",
    "section": "",
    "text": "Experiments with EconML and Causal Learning: Customer Segmentation\n\n\n\n\n\n\nHealth economics\n\n\n\n\n\n\n\n\n\nJan 31, 2025\n\n\nDrew Day\n\n\n\n\n\n\n\n\n\n\n\n\nCohort State-Transition Models (cSTM) for Cost Effectiveness Analysis\n\n\n\n\n\n\nHealth economics\n\n\n\n\n\n\n\n\n\nJan 17, 2025\n\n\nDrew Day\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Drew Day",
    "section": "",
    "text": "GitHub\n  \n  \n    \n     LinkedIn\n  \n  \n    \n     Google Scholar\n  \n  \n    \n     Full CV\n  \n\n  \n  \n\nGoals\nI want to be advancing the cutting edge of statistics and data science for personalizing medicine, discovering markers of disease, identifying efficacy and safety signals in real-world data, and optimizing the health and economic benefits of medications.\nThe purpose of this website is not just to introduce my work, but also to blog about the latest statistical and programming advances.\n\n\nExperience\nEpidemiologist - Seattle Children’s Research Institute - 3 years\nResearch Scientist III, Epidemiology - Seattle Children’s Research Institute - 3 years\nResearch Associate - Duke University Integrated Toxicology and Environmental Health - 5 years\n\n\nEducation\nPh.D. - Duke University - Integrated Toxicology and Environmental Health - Combined training in toxicology, statistics, and epidemiology\nB.A., B.A., B.S. - Penn State University Schreyer Honors College"
  },
  {
    "objectID": "pkgguides/DrewDayRFunctions_overview.html",
    "href": "pkgguides/DrewDayRFunctions_overview.html",
    "title": "DrewDayRFunctions: An Overview",
    "section": "",
    "text": "DrewDayRFunctions is an R package I created to share convenience functions that aid in epidemiologic and other statistical analyses, focusing on regressions, exploratory data analysis, visualizations, and mediation.\nClick on each tab for a brief description of what each main function does:\n\nGLMResultsGAMResultsContrastCoefficientsInteractionCoefPlotGenericDataSummaryMediationCurveGetExcelColors\n\n\nThis is a useful function for quickly running a whole bunch of linear regressions and getting useful plots and diagnostics out of them.\nThis runs many regressions by taking combinations of predictors, outcome, and covariates; performing linear regressions or generalized linear regressions for binary or count outcomes (logistic, log binomial, probit, Poisson, negative binomial, etc.); and returning 1. a table of coefficient and diagnostic values, 2. a summary plot, and 3. a list of all model objects for any follow-up analyses. Options include adding categorical or continuous interaction terms, using robust errors, performing multiple imputation using MICE, k-fold cross validation, and rerunning models when excluding high-leverage points.\nHere is an example plot output for models concerning car engines with continuous outcomes MPG and Horsepower, continuous predictors Weight, Displacement, 1/4 Mile Time, and Rear Axle Ratio; and a categorical interaction term VS (V-shaped engine or straight engine):\n\n\n\nThis is a useful function for running lots of GAMs and getting back useful visualizations and diagnostics.\nThis function takes combinations of predictor of interest, outcome, and covariate variables, performs linear or binary generalized additive models, and returns 1. a table of parameter and diagnostic values, 2. summary plot(s), and 3. a list of all model objects. Options include adding continuous and categorical variable by smooth interactions, performing smooth selection, and rerunning models when excluding upper/lower quantiles of data to examine edge effects on smooth fits.\nHere is some example output for a toy example with a continuous predictor (“X”) smooth interaction with the categorical variable sex (“S”). The red curve is for females, the blue curve is for males, and the green curve is the difference between those curves, each with effective degrees of freedom (EDF) and smooth p-values displayed:\n\n\n\nThis function extracts all the coefficients from regression models that have categorical variables with &gt;2 levels.\nThis extracts all the binary combinations of contrasts for a categorical (factor) predictor of interest and/or interaction term from a linear model. For example, if a predictor variable of interest is a 4-level categorical treatment variable with factor levels {“placebo”, “drug1”, “drug2”, “drug3”}, a typical linear model will only return the “drug1 - placebo”, “drug2 - placebo”, and “drug3 - placebo” binary contrast coefficients. This function will return all binary contrast coefficients for all possible referent levels, meaning the aforementioned coefficients will be returned, as well the coefficients for “drug2 - drug1”, “drug 3 - drug 1”, and “drug 3 - drug 2”. Furthermore, ContrastCoefficients can also provide all contrasts for a categorical interaction term as well, including all marginal coefficients for the main effects (e.g., predictor given interacting variable = level 1, predictor given interacting variable level 2, etc.). This works for all sorts of linear models and GLMs, including with MICE imputation.\nBelow is example output for a model regressing MPG for an engine on continuous weight (“wt”) and categorical number of engine cylinders (“cyl”; 4, 6, or 8):\n\n\n\nThis visualizes continuous by continuous interactions and was used for a 2024 Melamine paper in Environmental Research:\nThis is a function for visualizing continuous by continuous interactions from linear regression models. These plots show how the coefficient between one variable in a bivariate continuous interaction and the outcome changes over levels of the interacting variable.\nThe example below is from a toy dataset and a model regressing some outcome on the main effects and interaction of predictors “X1” and “X2”:\n\n\n\nThis generates an interactive HTML Markdown document to summarize all the variables in a dataset.\nThis function takes a data frame and outputs an HTML Markdown document that contains an interactive table with a data dictionary if one is provided, an interactive table of summary statistics for the continuous variables as well as boxplots and histograms for those variables, and a searchable table showing frequencies for all categorical variables that also uses a bar plot to visualize frequencies. This is an easy way to summarize a newly obtained dataset or to send a general summary to someone who will be receiving a dataset.\nBelow is a screenshot from an example document using the built-in “iris” dataset:\n\n\n\nThis function evaluates and visualizes nonlinear mediation.\nThis function extends the functionality of the ‘mediate’ function from the ‘mediation’ R package for causal mediation analysis as defined by Imai et al. 2010 for modelling linear and nonlinear mediation effects in the case of a continuous treatment. Mediation analysis is applied over a range of continuous treatment values to obtain a curve of the average causal mediation effect (ACME), average direct effect (ADE), total effect, and the proportion mediated. This is useful for characterizing nonlinear mediation effects.\nHere is example output from the associated plot.medcurve function. There is a parabolic curvilinear association between the exposure and mediator, and between the mediator and outcome. The red point is the comparison “control” treatment level, fixed at about the median. Each point and set of CIs reflect the ACME estimate when comparing a treatment at that level to the “control” treatment where the red point is (zero). Each point estimate is connected with a smooth fit to estimate the ACME function:\n\n\n\nThis function extracts and summarizes Excel spreadsheet background colors.\nThis function takes an Excel spreadsheet from a .xlsx file and creates a data frame where the values in the dataset are replaced with all the background colors in that Excel sheet. It also provides a table and heatmap plot showing how many cells have each color detected in the Excel spreadsheet. This function is useful for when important information is encoded in the background colors of an Excel file.\nHere is an example plot that summarizes all colors in a given spreadsheet and total cell counts with each color:"
  },
  {
    "objectID": "pkgguides/wqspt.html",
    "href": "pkgguides/wqspt.html",
    "title": "How to use the wqspt package",
    "section": "",
    "text": "Weighted quantile sum (WQS) regression is a statistical technique to evaluate the effect of complex exposure mixtures on an outcome (Carrico 2015). It is a single-index method which estimates a combined mixture sum effect as well as weights determining each individual mixture component’s contributions to the sum effect. However, the model features a statistical power and Type I error (i.e., false positive) rate tradeoff, as there is a machine learning step to determine the weights that optimize the linear model fit. If the full data is used to estimate both the mixture component weights and the regression coefficients, there is high power but also a high false positive rate since coefficient p-values are calculated for a weighted mixture independent variable with weights that have already been optimized to find a large effect.\nWe recently proposed alternative methods based on a permutation test that should reliably allow for both high power and low false positive rate when utilizing WQS regression. The permutation test is a method of obtaining a p-value by simulating the null distribution through permutations of the data. The permutation test algorithm is described more in detail and validated in Day et al. 2022. The version of this permutation test used for a continuous outcome variable has been applied in Loftus et al. 2021, Day et al. 2021, Wallace et al. 2022, Barrett et al. 2022, and Freije et al. 2022. Another version of the permutation test adapted for WQS logistic regression with a binary outcome variable is applied in Loftus et al. 2022.\n\n\nThe goal of WQS regression is to determine whether an exposure mixture is associated with an outcome in a prespecified direction. It fits the following model:\n\\(Y = \\beta_0 + \\beta_1(\\sum_{i=1}^{m} w_i {X_q}_i) + Z'\\gamma\\)\nWhere \\(Y\\) is the outcome variable, \\(\\beta_0\\) is the intercept, \\(\\beta_1\\) is the coefficient for the weighted quantile sum, \\(\\sum_{i=1}^{c} w_i {X_q}_i\\) is the weighted index for the set of quantiled mixture exposures, \\(Z\\) is the set of covariates, and \\(\\gamma\\) is the regression coefficients for the covariates.\nA full description of the WQS methodology is described in Carrico 2015.\n\n\n\nThe WQS regression comprises two steps, for which we typically split the data into a training and validation set. Doing this reduces statistical power since we are training our model on only part of the data. On the other hand, if we skip this training/test split, we can get a skewed representation of uncertainty for the WQS coefficient. A permutation test method gives us a p-value for the uncertainty while also allowing us to use the full dataset for training and validation. This p-value is based on comparing a test value (e.g., coefficient or naive p-values) to iterated values, and so the minimum non-zero p-value that can be detected by the permutation test would be 1 divided by the number of permutation test iterations. For example, if we run 200 iterations, we’d be able to define a p-value of as low as 1/200 = 0.005, and any lower p-value would appear as zero and be interpreted as &lt;0.005.\n\n\n\nRun WQS regression without splitting the data, obtaining a WQS coefficient estimate.\nRegress the outcome on all covariates but not the WQS variable. Then obtain the predicted outcome values and their residuals from this regression.\nRandomly permute the residual values and add them to the predicted outcome values to get a new outcome variable \\(y*\\).\nRun a WQS regression without splitting the data in which \\(y*\\) replaces the vector of observed outcome variables, obtaining an estimate for the WQS coefficient \\(\\beta_1^*\\).\nRepeat steps 3 and 4.\nCalculate the p-value by taking the proportion of \\(\\beta_1^*\\) values greater than the WQS coefficient estimate obtained in Step 1.\n\n\n\n\n\nRegress each of the \\(m\\) mixture components on all covariates \\(Z\\) and obtain a \\(n\\) observations x \\(m\\) matrix with columns being the residuals from each of the \\(m\\) models (\\(R_{m|Z}\\)).\nObtain the initial fit (\\(fit1\\)) by running a “non-split” WQS logistic regression (or other WQS GLM) in which the binary (or count) outcome variable \\(Y\\) is regressed on the WQS vector and the covariates, and the mixture matrix used to calculate the WQS vector is the matrix of residuals from Step 1, \\(R_{m|Z}\\).\nObtain the reduced fit (\\(fit2\\)) by running a logistic regression (or other GLM) regressing \\(Y\\) on \\(Z\\).\nCalculate the test p-value (\\(p_t\\)) as \\(1-pchisq(d(fit1)-d(fit2),1)\\) where d is the deviance for a given model and \\(pchisq(x,1)\\) is the probability density function of the chi-square distribution in which the input \\(x\\) is the difference between the deviances of \\(fit1\\) and \\(fit2\\) and there is 1 degree of freedom.\nPermute the rows of the \\(R_{m|Z}\\) residual matrix from Step 1 and repeat Step 2 to get a series of null fit1 models (\\(fit1^*\\)) for K iterations. Obtain a distribution of permuted p-values (\\(p^*\\)) using the following formula: \\(p^*=1-pchisq(fit1^*)-d(fit2),1\\)).\nObtain the number of permuted \\(p^*\\) less than or equal to the test \\(p_t\\) from Step 4 and divide that by the number of iterations K to calculate the permutation test p-value.\n\nNote that the above algorithm has been validated in WQS logistic regressions but not yet for other forms of WQS GLMs (e.g., WQS Poisson regression). However, since deviances can also be derived from those models, the algorithm should work for those other WQS GLMs as well."
  },
  {
    "objectID": "pkgguides/wqspt.html#about-wqs",
    "href": "pkgguides/wqspt.html#about-wqs",
    "title": "How to use the wqspt package",
    "section": "",
    "text": "The goal of WQS regression is to determine whether an exposure mixture is associated with an outcome in a prespecified direction. It fits the following model:\n\\(Y = \\beta_0 + \\beta_1(\\sum_{i=1}^{m} w_i {X_q}_i) + Z'\\gamma\\)\nWhere \\(Y\\) is the outcome variable, \\(\\beta_0\\) is the intercept, \\(\\beta_1\\) is the coefficient for the weighted quantile sum, \\(\\sum_{i=1}^{c} w_i {X_q}_i\\) is the weighted index for the set of quantiled mixture exposures, \\(Z\\) is the set of covariates, and \\(\\gamma\\) is the regression coefficients for the covariates.\nA full description of the WQS methodology is described in Carrico 2015."
  },
  {
    "objectID": "pkgguides/wqspt.html#permutation-test",
    "href": "pkgguides/wqspt.html#permutation-test",
    "title": "How to use the wqspt package",
    "section": "",
    "text": "The WQS regression comprises two steps, for which we typically split the data into a training and validation set. Doing this reduces statistical power since we are training our model on only part of the data. On the other hand, if we skip this training/test split, we can get a skewed representation of uncertainty for the WQS coefficient. A permutation test method gives us a p-value for the uncertainty while also allowing us to use the full dataset for training and validation. This p-value is based on comparing a test value (e.g., coefficient or naive p-values) to iterated values, and so the minimum non-zero p-value that can be detected by the permutation test would be 1 divided by the number of permutation test iterations. For example, if we run 200 iterations, we’d be able to define a p-value of as low as 1/200 = 0.005, and any lower p-value would appear as zero and be interpreted as &lt;0.005.\n\n\n\nRun WQS regression without splitting the data, obtaining a WQS coefficient estimate.\nRegress the outcome on all covariates but not the WQS variable. Then obtain the predicted outcome values and their residuals from this regression.\nRandomly permute the residual values and add them to the predicted outcome values to get a new outcome variable \\(y*\\).\nRun a WQS regression without splitting the data in which \\(y*\\) replaces the vector of observed outcome variables, obtaining an estimate for the WQS coefficient \\(\\beta_1^*\\).\nRepeat steps 3 and 4.\nCalculate the p-value by taking the proportion of \\(\\beta_1^*\\) values greater than the WQS coefficient estimate obtained in Step 1.\n\n\n\n\n\nRegress each of the \\(m\\) mixture components on all covariates \\(Z\\) and obtain a \\(n\\) observations x \\(m\\) matrix with columns being the residuals from each of the \\(m\\) models (\\(R_{m|Z}\\)).\nObtain the initial fit (\\(fit1\\)) by running a “non-split” WQS logistic regression (or other WQS GLM) in which the binary (or count) outcome variable \\(Y\\) is regressed on the WQS vector and the covariates, and the mixture matrix used to calculate the WQS vector is the matrix of residuals from Step 1, \\(R_{m|Z}\\).\nObtain the reduced fit (\\(fit2\\)) by running a logistic regression (or other GLM) regressing \\(Y\\) on \\(Z\\).\nCalculate the test p-value (\\(p_t\\)) as \\(1-pchisq(d(fit1)-d(fit2),1)\\) where d is the deviance for a given model and \\(pchisq(x,1)\\) is the probability density function of the chi-square distribution in which the input \\(x\\) is the difference between the deviances of \\(fit1\\) and \\(fit2\\) and there is 1 degree of freedom.\nPermute the rows of the \\(R_{m|Z}\\) residual matrix from Step 1 and repeat Step 2 to get a series of null fit1 models (\\(fit1^*\\)) for K iterations. Obtain a distribution of permuted p-values (\\(p^*\\)) using the following formula: \\(p^*=1-pchisq(fit1^*)-d(fit2),1\\)).\nObtain the number of permuted \\(p^*\\) less than or equal to the test \\(p_t\\) from Step 4 and divide that by the number of iterations K to calculate the permutation test p-value.\n\nNote that the above algorithm has been validated in WQS logistic regressions but not yet for other forms of WQS GLMs (e.g., WQS Poisson regression). However, since deviances can also be derived from those models, the algorithm should work for those other WQS GLMs as well."
  },
  {
    "objectID": "pkgguides/wqspt.html#wqs_pt",
    "href": "pkgguides/wqspt.html#wqs_pt",
    "title": "How to use the wqspt package",
    "section": "wqs_pt",
    "text": "wqs_pt\n\nArguments\nwqs_pt uses a gwqs object (from the gWQS package) as an input. To use wqs_pt, we first need to run an initial permutation test reference WQS regression run while setting validation = 0. Note that permutation test can currently take in gwqs inputs with the following families: family = gaussian(link = \"identity\"), family = binomial() with any accepted link function (e.g., “logit” or “probit”), family = poisson(link = \"log\"), family = quasipoisson(link = \"log\"), and family = \"negbin\" for negative binomial. It is not currently able to accommodate multinomial WQS regression, stratified weights, or WQS interaction terms.\nWe will use this gwqs object as the model argument for the wqs_pt function and set the following additional parameters:\n\nboots: Number of bootstraps for the WQS regression run in each permutation test iteration. Note that we may elect a bootstrap count boots lower than that specified in the model object for the sake of efficiency. If we do, wqs_pt will run the iterated WQS regressions for the permutation test with the number of bootstraps defined in boots. If boots is not specified, then the function will use the same bootstrap count in the permutation test iterated WQS regressions as that specified in the main WQS regression.\nniter: Number of permutation test iterations.\nb1_pos: A logical value that indicates whether beta values should be positive or negative.\nrs: A logical value indicating whether the random subset implementation for WQS should be performed (Curtin 2019)\nplan_strategy: Evaluation strategy for the plan function (“sequential”, “transparent”, “multisession”, “multicore”, “multiprocess”, “cluster”, or “remote”). See the documentation for the future::plan function for full details.\n\nseed: Random seed for the permutation test WQS reference run\n\nThe arguments b1_pos and rs should be consistent with the inputs chosen in the model object. The seed should ideally be consistent with the seed set in the model object, though this is not required.\n\n\nOutputs\nThe permutation test returns an object of class wqs_pt, which contains three sublists:\n\nperm_test\n\npval: permutation test p-value\nLinear WQS regression only\ntestbeta: reference WQS coefficient \\(\\beta_1\\) value\nbetas: a vector of \\(\\beta_1\\) values from each iteration of the permutation test\nWQS GLM only\ntestpval: test reference p-value\npermpvals: p-values from each iteration of the permutation test\n\ngwqs_main: main gWQS object (same as model input)\ngwqs_perm: permutation test reference gWQS object (NULL if model family != \"gaussian\" or if same number of bootstraps are used in permutation test WQS regression runs as in the main run.)\n\n\n\nPlotting method\nThe wqs_pt class has a wqspt_plot method to help visualize and summarize WQS permutation test results. Plots include (1) a forest plot of the beta WQS coefficient with the naive confidence intervals as well as the permutation test p-value and (2) a heatmap of the WQS weights for each mixture component."
  },
  {
    "objectID": "pkgguides/wqspt.html#wqs_full_perm",
    "href": "pkgguides/wqspt.html#wqs_full_perm",
    "title": "How to use the wqspt package",
    "section": "wqs_full_perm",
    "text": "wqs_full_perm\nThe second function wqs_full_perm is a full wrapper which implements the initial WQS regression run using gWQS::gwqs and the permutation test in one function call.\nTo use wqs_full_perm, you must specify the same required arguments as needed in the gwqs call. This function can run WQS regressions and the permutation test for the following families: family = gaussian(link = \"identity\"), family = binomial() with any accepted link function (e.g., “logit” or “probit”), family = poisson(link = \"log\"), family = quasipoisson(link = \"log\"), and family = \"negbin\" for negative binomial. wqs_full_permis not currently able to accommodate multinomial WQS regression, stratified weights, or WQS interaction terms.\nFor the bootstrap count b argument, you must specify b_main,the number of bootstraps for the main WQS regression run, as well as b_perm, the number of bootstraps for the permutation test reference WQS regression run (linear WQS regression only) and each WQS regression iteration of the permutation test. As with before, you can choose to set b_main \\(&gt;\\) b_perm for the sake of efficiency. Finally, you should indicate the number of desired permutation test runs niter.\nSince the WQS permutation test can be computationally intensive, you can specify stop_if_nonsig = TRUE if you do not wish for the permutation test to proceed if the naive main WQS regression run produces an nonsignificant result (if the p-value is below the stop_thresh argument, for which the default is 0.05). See Recommendations for Use section below.\nThe wqs_full_perm returns an object of class wqs_pt, with outputs described above."
  },
  {
    "objectID": "pkgguides/wqspt.html#recommendations-for-use",
    "href": "pkgguides/wqspt.html#recommendations-for-use",
    "title": "How to use the wqspt package",
    "section": "Recommendations for Use",
    "text": "Recommendations for Use\nLarger bootstrap counts and numbers of iterations lead to better estimates, though it is unclear how many iterations or bootstraps are needed for a stable estimate. We generally recommend using 1000 bootstraps on the main WQS regression and then performing 200 iterations of 200-boostrap WQS regressions for the permutation test. However, this takes a substantial amount of computational time, and one could also get relatively stable p-values for instance for 100 iterations of 100-boostrap WQS regressions for the permutation test.\nWe recommend that users only apply the permutation test in cases where the naive WQS test approaches significance or near-significance. If the naive test produces a non-significant result, then there likely is no reason to run the permutation test, as it will produce a result which is more conservative than the naive method (i.e., it will have a larger p-value). This is the strategy that we have applied in our published papers (Loftus et al. 2021 and Day et al. 2021)."
  },
  {
    "objectID": "pkgguides/wqspt.html#example-1-using-wqs_pt",
    "href": "pkgguides/wqspt.html#example-1-using-wqs_pt",
    "title": "How to use the wqspt package",
    "section": "Example 1 (using wqs_pt)",
    "text": "Example 1 (using wqs_pt)\nThis is an example where the WQS permutation test confirms a significant naive result.\nWe first load both the wqspt and gWQS packages.\n\nlibrary(gWQS)\nlibrary(wqspt)\n\nThen we produce a simulated dataset with the following parameters:\n\nWQS coefficient \\(\\beta_1\\): 0.2\nMixture weights: 0.15 for first 5 components, 0.05 for remaining 5 components\n\n\n# simulated dataset\nsim_res1 &lt;- wqs_sim(nmix = 10,\n                    ncovrt = 10,\n                    nobs = 1000,\n                    ntruewts = 10, \n                    ntruecovrt = 5, \n                    truewqsbeta = 0.2, \n                    truebeta0 = 2, \n                    truewts = c(0.15, 0.15, 0.15, 0.15, 0.15,\n                                0.05, 0.05, 0.05, 0.05, 0.05), \n                    q = 10, \n                    seed = 16)\n\nsim_data1 &lt;- sim_res1$Data\n\nwqs_form &lt;- formula(paste0(\"y ~ wqs + \", paste(paste0(\"C\",1:10), collapse=\"+\")))\n\nNow we run WQS regression on the simulated data.\n\n# mixture names\nmix_names1 &lt;- colnames(sim_data1)[2:11]\n\n# create reference wqs object\nwqs_main1 &lt;- gwqs(wqs_form, mix_name = mix_names1, data = sim_data1, q = 10, validation = 0,\n                  b = 20, b1_pos = T, plan_strategy = \"multicore\", family = \"gaussian\", \n                  seed = 16)\n\nFinally, we can perform a permutation test on the WQS object.\n\n# run permutation test\nperm_test_res1 &lt;- wqs_pt(wqs_main1, niter = 50, boots = 5, b1_pos = T, seed = 16)\n\nNote that the naive WQS regression produces a significant result for the WQS coefficient (p-value &lt; 0.001).\n\nmain_sum1 &lt;- summary(perm_test_res1$gwqs_main)\n\n\nmain_sum1$coefficients\n#&gt;                 Estimate Std. Error     t value      Pr(&gt;|t|)\n#&gt; (Intercept)  1.716588215 0.13195745  13.0086490  7.960621e-36\n#&gt; wqs          0.271247707 0.02846484   9.5292204  1.183377e-20\n#&gt; C1           0.915972918 0.03235401  28.3109569 1.372658e-129\n#&gt; C2           1.837398541 0.03166084  58.0337820 1.460458e-320\n#&gt; C3          -1.567906582 0.03096844 -50.6291836 9.835375e-277\n#&gt; C4          -0.261844308 0.03025602  -8.6542893  1.987022e-17\n#&gt; C5          -0.350600283 0.03111404 -11.2682323  8.594271e-28\n#&gt; C6           0.017181769 0.03214707   0.5344739  5.931340e-01\n#&gt; C7           0.028020482 0.03007333   0.9317386  3.516993e-01\n#&gt; C8           0.006594393 0.03040937   0.2168540  8.283669e-01\n#&gt; C9          -0.075174923 0.03029635  -2.4813194  1.325512e-02\n#&gt; C10         -0.003960226 0.03079737  -0.1285898  8.977084e-01\n\nThe permutation test confirms the significance of this result.\n\nperm_test_res1$perm_test$pval\n#&gt; [1] 0\n\nHere are the summary plots:\n\nwqspt_plot(perm_test_res1)$FullPlot\n#&gt; Warning in get_plot_component(plot, \"guide-box\"): Multiple components found;\n#&gt; returning the first one. To return all, use `return_all = TRUE`."
  },
  {
    "objectID": "pkgguides/wqspt.html#example-2-using-wqs_pt",
    "href": "pkgguides/wqspt.html#example-2-using-wqs_pt",
    "title": "How to use the wqspt package",
    "section": "Example 2 (using wqs_pt)",
    "text": "Example 2 (using wqs_pt)\nThis is an example where the WQS permutation test goes against a (false positive) significant naive result.\nWe produce a simulated dataset with the following parameters:\n\nWQS coefficient \\(\\beta_1\\): 0\nMixture weights: 0.15 for first 5 components, 0.05 for remaining 5 components\n\n\nsim_res2 &lt;- wqs_sim(nmix = 10,\n                    ncovrt = 10,\n                    nobs = 1000,\n                    ntruewts = 10, \n                    ntruecovrt = 5, \n                    truewqsbeta = 0, \n                    truebeta0 = 0.1, \n                    truewts = c(0.15, 0.15, 0.15, 0.15, 0.15,\n                                0.05, 0.05, 0.05, 0.05, 0.05), \n                    q = 10, \n                    seed = 16)\n\nsim_data2 &lt;- sim_res2$Data\n\nNow we run WQS regression as well as the permutation test on the simulated data.\n\n# mixture names\nmix_names2 &lt;- colnames(sim_data2)[2:11]\n\n# create reference wqs object\nwqs_main2 &lt;- gwqs(wqs_form, mix_name = mix_names2, data = sim_data2, q = 10, validation = 0,\n                  b = 20, b1_pos = T, plan_strategy = \"multicore\", family = \"gaussian\", \n                  seed = 16)\n\n# run permutation test\nperm_test_res2 &lt;- wqs_pt(wqs_main2, niter = 50, boots = 5, b1_pos = T, seed = 16)\n\nNote that the naive WQS regression produces a significant result for the WQS coefficient (p-value = 0.002).\n\nmain_sum2 &lt;- summary(perm_test_res2$gwqs_main)\n\n\nmain_sum2$coefficients\n#&gt;                 Estimate Std. Error     t value      Pr(&gt;|t|)\n#&gt; (Intercept) -0.242259146 0.12595439  -1.9233879  5.471846e-02\n#&gt; wqs          0.084337304 0.02709982   3.1120982  1.910997e-03\n#&gt; C1           0.916079837 0.03236572  28.3040164 1.530151e-129\n#&gt; C2           1.836539134 0.03166066  58.0069707 2.079028e-320\n#&gt; C3          -1.568725043 0.03099602 -50.6105377 1.279103e-276\n#&gt; C4          -0.260464045 0.03027002  -8.6046855  2.973949e-17\n#&gt; C5          -0.350431787 0.03114219 -11.2526386  1.005351e-27\n#&gt; C6           0.016352846 0.03216484   0.5084075  6.112811e-01\n#&gt; C7           0.027852144 0.03008416   0.9258077  3.547720e-01\n#&gt; C8           0.006968031 0.03042095   0.2290537  8.188746e-01\n#&gt; C9          -0.074755354 0.03031780  -2.4657251  1.384261e-02\n#&gt; C10         -0.003661685 0.03081250  -0.1188377  9.054281e-01\n\nThe permutation test, however, repudiates the significance of these plots (p = 0.12).\n\nperm_test_res2$perm_test$pval\n#&gt; [1] 0.12\n\nHere are the summary plots:\n\nwqspt_plot(perm_test_res2)$FullPlot\n#&gt; Warning in get_plot_component(plot, \"guide-box\"): Multiple components found;\n#&gt; returning the first one. To return all, use `return_all = TRUE`."
  },
  {
    "objectID": "pkgguides/wqspt.html#example-3-using-wqs_full_perm",
    "href": "pkgguides/wqspt.html#example-3-using-wqs_full_perm",
    "title": "How to use the wqspt package",
    "section": "Example 3 (using wqs_full_perm)",
    "text": "Example 3 (using wqs_full_perm)\nUsing the same data as in Example 1, we run the WQS regression with permutation test using the full wrapper wqs_full_perm call.\n\nperm_test_res3 &lt;- wqs_full_perm(wqs_form,\n                               data = sim_data1,\n                               mix_name = mix_names1,\n                               q = 10,\n                               b_main = 20,\n                               b_perm = 5,\n                               b1_pos = T,\n                               niter = 50,\n                               seed = 16,\n                               plan_strategy = \"multicore\")\n\n\nwqspt_plot(perm_test_res3)$FullPlot\n#&gt; Warning in get_plot_component(plot, \"guide-box\"): Multiple components found;\n#&gt; returning the first one. To return all, use `return_all = TRUE`."
  },
  {
    "objectID": "pkgguides/wqspt.html#example-4-using-wqs_full_perm-on-binary-outcome-example",
    "href": "pkgguides/wqspt.html#example-4-using-wqs_full_perm-on-binary-outcome-example",
    "title": "How to use the wqspt package",
    "section": "Example 4 (using wqs_full_perm on binary outcome example)",
    "text": "Example 4 (using wqs_full_perm on binary outcome example)\nThis is an example in which we apply the logistic regression version of the WQS permutation test.\nWe produce a simulated dataset with the following parameters:\n\nWQS coefficient \\(\\beta_1\\): 0.4\nMixture weights: 0.15 for first 5 components, 0.05 for remaining 5 components\n\n\nsim_res3 &lt;- wqs_sim(nmix = 10,\n                    ncovrt = 10,\n                    nobs = 1000,\n                    ntruewts = 10, \n                    ntruecovrt = 5, \n                    truewqsbeta = 0.4, \n                    truebeta0 = -2.5, \n                    truewts = c(0.15, 0.15, 0.15, 0.15, 0.15,\n                                0.05, 0.05, 0.05, 0.05, 0.05), \n                    q = 10, \n                    family = \"binomial\",\n                    seed = 16)\n\nsim_data3 &lt;- sim_res3$Data\n\nperm_test_res4 &lt;- wqs_full_perm(wqs_form,\n                               data = sim_data3,\n                               mix_name = mix_names1,\n                               q = 10,\n                               b_main = 20,\n                               b_perm = 5,\n                               b1_pos = T,\n                               niter = 50,\n                               seed = 16,\n                               plan_strategy = \"multicore\",\n                               family = \"binomial\")\n\n\nwqspt_plot(perm_test_res4)$FullPlot\n#&gt; Warning in get_plot_component(plot, \"guide-box\"): Multiple components found;\n#&gt; returning the first one. To return all, use `return_all = TRUE`."
  },
  {
    "objectID": "posts/cstm_healthecon.html",
    "href": "posts/cstm_healthecon.html",
    "title": "Cohort State-Transition Models (cSTM) for Cost Effectiveness Analysis",
    "section": "",
    "text": "Introduction\nA common tool used to project the long-term outcomes and costs of various policies, treatments, or other interventions in health economic analysis is the cohort state-transition model (cSTM). These models predict how a individuals in a hypothetical cohort transition between states over time, often health states when used in the context of health economics. cSTM are appropriate tools to use when one wants to model a dynamic process (i.e., evolving over time) in which there are transitions between discrete states. These models are often used to compare the costs and benefits of alternative interventions, and we will run through some examples of how to apply these models to these types of questions using R.\nThe “cohort” part of cSTM indicates that each theoretical cohort is considered homogeneous. There is another form of STM called individual STMs (iSTMs) that are more computationally demanding but can capture how time-varying variables impact the state-transition probabilities. We will focus solely on the simpler cSTMs for this post.\nThis document will pull largely from a tutorial in using cSTM for cost-effectiveness analysis (CEA) published by Alarid-Escudero et al. out of the Center for Research and Teaching in Economics (CIDE) in Mexico in 2022 (their code repo here). It will cover both time-independent and time-dependent forms of cSTM, and the later parts of the document will also cover later developments in state-transition models and their applications to economic analysis.\nAdditional discussion of best practices for STMs can be found in State-Transition Modeling: A Report of the ISPOR-SMDM Modeling Good Research Practices Task Force-3\n\n\nTime-Independent cSTM: The Math\n\nBasic concepts\nSTMs model how the number of individuals in each of a set of states evolves over time. These states are a finite set of mutually exclusive (i.e., no overlap) and completely exhaustive (i.e., no unmeasured/unobserved states) discrete values. In the case of discrete time steps, the states of a given time can be represented by a “state vector” \\(m\\) with individual entries for each of the number of states \\(n_s\\):\n\\[Equation 1: m_t=[m_{(t, 1)}, m_{(t, 2)}, ..., m_{(t, n_s)}]\\]\n\\(n_s\\) mutually exclusive states across \\(n_t\\) mutually exclusive discrete time cycles, usually of a fixed length (e.g., months or years), wherein individuals in each state are indistinguishable from one another. Probabilities \\(p\\) are assigned for transitioning between each combination of two states or staying within the same state over a given time cycle length. These probabilities are Markovian, which means that the probabilities are only dependent on the current state. This is why these models are also sometimes called “Markov models”.\nAs an example, if the probability of transitioning from the state “healthy” to the state “sick” is 0.02 over a one-year cycle, this means that the model assumes that everyone in the “healthy” state has the same 0.02 probability of transitioning into the “sick” state no matter at what time during that year or if they had been in the “sick” state prior to being in the “healthy” state or how long they’ve been in the “healthy” state, etc. The model simply assumes that the only factor impacting the probability is that current state.\nA popular STM framework common in CEA is the Decision Analysis in R for Technologies in Health (DARTH) 4-state (\\(n_s=4\\)) “Sick-Sicker Model”. Here patients transition between the states “Healthy (H)”, “Sick (S1)”, “Sicker (S2)”, and “Dead (D)”, with participants who reach the “Sicker” S2 state only either staying in that state or dying. Fig. 1 below shows how the states evolve, with probabilities of each transition being marked as \\(p_{\\zeta_1\\zeta_2}\\) where each \\(\\zeta_1\\) is starting state and \\(\\zeta_2\\) is the ending state. Not shown are the probabilities of remaining in the same state, for example the probability of remaining dead (\\(p_{DD}\\)), which is 100%.\n\n\n\nFig. 1: DARTH Sick-Sicker Model"
  },
  {
    "objectID": "projects.html#project-2",
    "href": "projects.html#project-2",
    "title": "Projects",
    "section": "Project 2",
    "text": "Project 2"
  },
  {
    "objectID": "projects.html#project-3",
    "href": "projects.html#project-3",
    "title": "Projects",
    "section": "Project 3",
    "text": "Project 3"
  }
]